AI芯片大部分指GPU；

Turing 架构（2018年）：T4
Ampere Architecture (2020年)：A10
Hopper架构（2022年，AI训练数据中心）：H100、H200、H20（H200的阉割版）
Ada Lovelace架构（2022年）：L20、L40（平衡AI推理与强大的图形渲染能力,升级版是L40S）、GeForce RTX3060(不同于其他的数据中心，这是消费级显卡、便宜理论性能大致接近T4)
Blackwell架构（2024年）

- 美国的出口管制
2022年起，美国以“国家安全”为由，多次升级出口管制，直接把英伟达H100、A100等顶级芯片挡在中国门外。为了合规，英伟达曾推出A800、H800、H20等“特供版”芯片，但性能均被阉割。
2025年4月，美国进一步加码管制，连H20这种降配版芯片都被迫停售；
2025年12月，美政府宣布调整出口管制政策，允许英伟达向中国出口H200人工智能芯片但实施较高的分成机制；更先进的Blackwell系列仍在禁售清单里；
25年国产GPU在国内市场份额已超英伟达，华为昇腾就占了40%；
第三方转运的方式有风险并且不可能规模很大；

- CUDA（Compute Unified Device Architecture）
是由NVIDIA公司创立的基于他们公司生产的GPU显卡的一个并行计算平台和编程模型；

```
cuda编程的概念：
- 设备：GPU及GPU的显存；
```