近似最近邻（Approximate Nearest Neighbor），相对有KNN（最近邻搜索/K-Nearest Neighbor Search）是一种基于暴力搜索的方法

- SIFT1M：广泛用于评估ANN搜索算法性能的基准数据集。基于 SIFT（Scale-Invariant Feature Transform）算法提取的，每个特征向量是一个 128 维的 SIFT 描述子，规模：
```
数据库集：包含 1,000,000 个特征向量（即1M）。
查询集：包含 10,000 个查询向量。
验证集：包含 100 个真实的最近邻向量（ground truth）。
```
- SIFT：图像特征提取算法（尺度不变特征变换），能够从图像中提取一组稳定的特征点（keypoints）及其对应的描述子（descriptors）。这些描述子是高维向量（通常是 128 维）
- HNSW（Hierarchical Navigable Small World）是一种高效的ANN搜索算法，基于分层的图结构设计；相关参数（M：每个节点在检索构图中可以连接多少个邻居节点；construction：写入数据时寻找邻居遍历的范围；ef：检索数据时动态候选元素集合的大小，并不是返回数量的limit参数）
- FLAT：平的，暴力搜索；
- IVF（Inverted File）是将高维空间划分为多个聚类，并为每个聚类构建一个倒排文件，检索时，输入向量会在最相似的聚类中心进行检索，从而提高检索效率；相关参数（nlist：表示聚簇中心个数；M：IVF_PQ索引中拆成子向量的个数；nprobe：表示一次查询几个最近的簇）
- IVF_FLAT：基于IVF，查询时遍历其聚类；
- IVF_PQ：对原始向量做了乘积量化（Product Quantization），将高维向量分割成多个子向量，并对每个子向量进行聚类，生成码本，然后将原始向量映射为一组离散的码本索引；例如x=[1.2,3.4,5.6,7.8,2.1,4.3,6.5,8.7]，切成两个子向量，最后只需要存储[2, 4]，2和4分别表示在码本中的下标，第一个子向量近似等于码本1中的第2个向量，第二个子向量近似等于码本2中的第4个向量；（PQ128表示把原向量切分成128个子向量？）
- IVF_SQ4/IVF_SQ8/IVF_SQ16：把原向量做了标量量化（Scalar Quantization），例如float32浮点数变成16bit（是搞成16位整数？还是归一化到0~1之间？）就是IVF_SQ16，变成4bit（只能表示16种取值）就是IVF_SQ4；
- Milvus：是一个高性能的向量数据库；
- FAISS（Facebook AI Similarity Search）是由 Facebook AI Research 开发的用于ANN搜索和稠密向量的库开源库，支持多种索引结构（如 FLAT、IVF、HNSW 等），存索引，不存向量？
- dense（稠密）向量，向量中的大多数元素都不是零；
- sparse（稀疏）向量，向量中的大多数元素都是零；通常应用场景是传统的机器学习方法（如 TF-IDF 表示的文本特征，假设词汇表是 ["machine", "AI", "data"]，一个文档的 TF-IDF 向量可能是 [0.5, 0.0, 0.0]，表示出现了一些machine，但是没有出现AI和data；这个特征可以用来文档分类/垃圾邮件过滤、文档检索）；
- TF-IDF（Term Frequency-Inverse Document Frequency）：该文档的该词频*所有文档中该词的稀有程度；
- K-Means 聚类：目标是将数据点分成 K 个簇，使得每个簇内的数据点尽可能接近；随机选K个点，然后分配点后，计算每个簇的平均值作为新的K个点，不断重复；
- DBSCAN（Density-Based Spatial Clustering of Applications with Noise）：是基于密度的聚类算法；不预设K，直接找如果一个点的邻域内（半径r范围）包含至少 minPts 个点，则该点是核心点，周围点加入该簇，不属于任何簇的点被标记为噪声；该算法复杂度较高，处理大规模数据可能较慢；
- RaBitQ1：将每个维度的浮点值压缩为 1bit，查询吞吐量显著提升，在 Milvus 等向量数据库中有成功应用；
- LUT：lookuptable（差表），是一种加速计算方法，例如rabitq-fastscan就是把每四维（相当于4bit）的可能距离都计算一遍（相当于16种取值），然后遍历候选集的时候，不再需要乘法来计算距离了，直接查表累加；
- 内积（IP/Inner Product），两个向量的点积 = a1*b1 + a2*b2 + ……，值越大表示两个向量越相似；
- 欧式距离（L2/Euclidean distance/欧几里得距离），空间距离 = （(a1-b1)^2+(a2-b2)^2+……）^(1/2)，值越小（接近0）表示两个向量越相似
- 余弦相似度（COSINE/Cosine Similarity），不考虑两个向量的大小，只考虑方向上的相似，取值范围[-1,1]，值越大（1）表示两个向量越同向，如果两个向量各自做归一化后（模长为1，例如(0.8,0.6)就是一个二维归一化向量），内积等于余弦相似度；
- 汉明距离，两个向量逐位比较是否相同，值越小（接近0）表示两个向量越相似；
- 稀疏向量索引，在文本检索的场景中，稀疏向量中的每个非空值，都可以理解成一个词的权重分数，假如设定了一千个关键词，那每个文章都可以生成一个一千维的向量，表示这边文章出现每个词的权重，在构建索引时，可以通过倒排的方式构建即一千个倒排链，在检索时输入向量可能只有两个关键字，则只在这两个倒排上进行检索；混合使用语义检索（稠密向量）和关键字检索（稀疏向量）可以进一步改善检索效果；
- RVQ(Residual Vector Quantization/残差向量量化)，多层量化，一个多维向量，通过第一层（提前训练的码本）找到初步相似的假如是3，然后相减得到就是残差，再拿这个残差去第二层找到相似的，假如是7，那么这个多维向量就可以用[3,7]表示；这样得到的量化信息相比PQ感觉优势在于：PQ的分隔可能不合理，例如24维只是暴力分成了3个8维子向量，如果某一维值比较小，容易被其他维度干扰，而RVQ经过一层残差后第二层大家的值都很小，就能突出效果；